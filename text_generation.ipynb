{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Text generation using a RNN ‚úçÔ∏è "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ Our goal is to use a dataset of Shakespeare's writing from http://karpathy.github.io/2015/05/21/rnn-effectiveness/ in order to generate Shakespeare like texts from our own prompts! **Our model will take in 100 characters and predict the 101st character.** To predict an entire paragraph we can call our model over and over again using our generated characters (i.e character 2-100 + our generated 101 to predict 102)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## 1Ô∏è‚É£ Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "### 1.1) Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:11.935727Z",
     "iopub.status.busy": "2022-12-14T13:32:11.935518Z",
     "iopub.status.idle": "2022-12-14T13:32:13.873227Z",
     "shell.execute_reply": "2022-12-14T13:32:13.872259Z"
    },
    "id": "yG_n40gFzf9s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 17:11:19.988266: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 17:11:20.157889: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-30 17:11:20.164513: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:20.164534: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-30 17:11:20.198788: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-30 17:11:21.157767: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:21.157871: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:21.157879: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "### 1.2) Get the data üìï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the helper function below üëá you can see it downloads us the data in the filename **shakespeare.txt** and returns us the file path to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:13.877626Z",
     "iopub.status.busy": "2022-12-14T13:32:13.876904Z",
     "iopub.status.idle": "2022-12-14T13:32:13.931441Z",
     "shell.execute_reply": "2022-12-14T13:32:13.930866Z"
    },
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path_to_data = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "### 1.3) Have a look at the data üîé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can open the file and read it as a string (we have to decode it to make a string rather than a byte string): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:13.934985Z",
     "iopub.status.busy": "2022-12-14T13:32:13.934481Z",
     "iopub.status.idle": "2022-12-14T13:32:13.939701Z",
     "shell.execute_reply": "2022-12-14T13:32:13.939141Z"
    },
    "id": "aavnuByVymwK"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_data, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:13.942844Z",
     "iopub.status.busy": "2022-12-14T13:32:13.942304Z",
     "iopub.status.idle": "2022-12-14T13:32:13.945817Z",
     "shell.execute_reply": "2022-12-14T13:32:13.945271Z"
    },
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## 2Ô∏è‚É£ Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### 2.1) Vectorize the text\n",
    "\n",
    "Before training, you need to convert the strings to a numerical representation. \n",
    "\n",
    "The [tf.keras.layers.StringLookup](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StringLookup) layer can convert each character into a numeric ID. This layer just needs the text to be split into tokens first. You can use the helper function [tf.strings.unicode_split](https://www.tensorflow.org/api_docs/python/tf/strings/unicode_split) to achieve that like the example below üëá."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With UTF-8 encoding, you can work with texts that contain characters from different languages, symbols, emojis, and other Unicode characters, ensuring compatibility and accurate representation of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:13.966911Z",
     "iopub.status.busy": "2022-12-14T13:32:13.966697Z",
     "iopub.status.idle": "2022-12-14T13:32:17.404330Z",
     "shell.execute_reply": "2022-12-14T13:32:17.403658Z"
    },
    "id": "a86OoYtO01go"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 17:11:23.403958: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:06:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-30 17:11:23.404106: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:23.404165: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:23.404207: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:23.404248: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:23.404289: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:23.404334: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:23.404374: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:23.404412: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-05-30 17:11:23.404420: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-30 17:11:23.410122: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Generate the vocab üìñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Generate a list of **unique characters** in our text and save it in the variable **`vocab`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(vocab := sorted(set(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s4f1q3iqY8f"
   },
   "source": [
    "‚ùì Now create the `tf.keras.layers.StringLookup` layer and save it as `ids_from_chars`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.408060Z",
     "iopub.status.busy": "2022-12-14T13:32:17.407548Z",
     "iopub.status.idle": "2022-12-14T13:32:17.423673Z",
     "shell.execute_reply": "2022-12-14T13:32:17.423064Z"
    },
    "id": "6GMlCe3qzaL9",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.preprocessing.string_lookup.StringLookup object at 0x7f137b7c2680>\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "print(ids_from_chars := StringLookup(vocabulary=vocab, mask_token=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary markdown='span'>üí° If you get stuck</summary>\n",
    "\n",
    "```python\n",
    "tf.keras.layers.StringLookup(vocabulary=vocab, mask_token=None)\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZmX_jbgQqfOi"
   },
   "source": [
    "It converts from tokens to character IDs based on the vocab we passed to it. \n",
    "\n",
    "‚ùì Use the layer below üëá and edit `chars` variable above and see what happens when you add characters outside the vocab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.427404Z",
     "iopub.status.busy": "2022-12-14T13:32:17.426909Z",
     "iopub.status.idle": "2022-12-14T13:32:17.434684Z",
     "shell.execute_reply": "2022-12-14T13:32:17.434039Z"
    },
    "id": "WLv5Q_2TC2pc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "To generate text, it will also be important to **invert this representation** and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uenivzwqsDhp"
   },
   "source": [
    "‚ùóÔ∏è Here instead of passing the original vocabulary generated with `sorted(set(text))`, use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` to get the vocabulary assigned to the previous `ids_from_chars` layer. \n",
    "\n",
    "This way, we also have a `[UNK]` string for unknown characters outside our original representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.438253Z",
     "iopub.status.busy": "2022-12-14T13:32:17.437740Z",
     "iopub.status.idle": "2022-12-14T13:32:17.449424Z",
     "shell.execute_reply": "2022-12-14T13:32:17.448738Z"
    },
    "id": "Wd2m3mqkDjRj"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqTDDxS-s-H8"
   },
   "source": [
    "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.452733Z",
     "iopub.status.busy": "2022-12-14T13:32:17.452253Z",
     "iopub.status.idle": "2022-12-14T13:32:17.457574Z",
     "shell.execute_reply": "2022-12-14T13:32:17.456992Z"
    },
    "id": "c2GCh0ySD44s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FeW5gqutT3o"
   },
   "source": [
    "‚úçÔ∏è We use `tf.strings.reduce_join` to join the characters back into strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.461040Z",
     "iopub.status.busy": "2022-12-14T13:32:17.460453Z",
     "iopub.status.idle": "2022-12-14T13:32:17.506637Z",
     "shell.execute_reply": "2022-12-14T13:32:17.505960Z"
    },
    "id": "zxYI-PeltqKP"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Define a function `text_from_ids` that takes a tensor of ids and returns the corresponding text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.509989Z",
     "iopub.status.busy": "2022-12-14T13:32:17.509440Z",
     "iopub.status.idle": "2022-12-14T13:32:17.512781Z",
     "shell.execute_reply": "2022-12-14T13:32:17.512180Z"
    },
    "id": "w5apvBDn9Ind",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Run the **test** below to check you are able to go back and forth from **text -> ids -> text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/eren/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/eren/code/erenkiliclar/data-text-generation/tests\n",
      "plugins: anyio-3.6.2, asyncio-0.19.0\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_helpers.py::TestHelpers::test_chars_to_ids \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 50%]\u001b[0m\n",
      "test_helpers.py::TestHelpers::test_ids_to_chars \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.28s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/helpers.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed helpers step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "test_texts = ['LeWagon', 'NLP`']\n",
    "test_chars = tf.strings.unicode_split(test_texts, input_encoding='UTF-8')\n",
    "test_ids = ids_from_chars(test_chars)\n",
    "test_reverse = text_from_ids(test_ids)\n",
    "result = ChallengeResult('helpers', ids=list(test_ids[0].numpy()), chars=test_reverse[1].numpy())\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) The dataset üöö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì First split our whole text using `unicode_split` and convert them all with `ids_from_chars`, to get all of our text as a single continuous array saved as `all_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.516290Z",
     "iopub.status.busy": "2022-12-14T13:32:17.515731Z",
     "iopub.status.idle": "2022-12-14T13:32:17.862389Z",
     "shell.execute_reply": "2022-12-14T13:32:17.861710Z"
    },
    "id": "UopbsKi88tm5",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "my_char = tf.strings.unicode_split(text, input_encoding=\"UTF-8\")\n",
    "print(all_ids := ids_from_chars(my_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then make a tensorflow dataset object with that array. This is an object which allows us to write pipelines to transform our data into the format needed for our model to read it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.866209Z",
     "iopub.status.busy": "2022-12-14T13:32:17.865672Z",
     "iopub.status.idle": "2022-12-14T13:32:17.870573Z",
     "shell.execute_reply": "2022-12-14T13:32:17.869940Z"
    },
    "id": "qmxrYDCTy-eL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>\n"
     ]
    }
   ],
   "source": [
    "print(ids_dataset := tf.data.Dataset.from_tensor_slices(all_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "The `batch` method allows us to set how many characters we should take at a time! In our case we want **101**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.909371Z",
     "iopub.status.busy": "2022-12-14T13:32:17.908760Z",
     "iopub.status.idle": "2022-12-14T13:32:17.926465Z",
     "shell.execute_reply": "2022-12-14T13:32:17.925781Z"
    },
    "id": "BpdjRO2CzOfZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(101, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PHW902-4oZt"
   },
   "source": [
    "It's easier to see  if we join the tokens back into strings üëá:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.929732Z",
     "iopub.status.busy": "2022-12-14T13:32:17.929193Z",
     "iopub.status.idle": "2022-12-14T13:32:17.944697Z",
     "shell.execute_reply": "2022-12-14T13:32:17.943979Z"
    },
    "id": "QO32cMWu4a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "For training you'll need a dataset of `(input, label)` pairs, where `input` and \n",
    "`label` are sequences. \n",
    "\n",
    "Even though we are predicting one character at a time, the sequence at each time step consists of the:\n",
    "\n",
    "1. `input` which is the `n` characters in the sequence up to the `n+1` character we want to predict\n",
    "2. `label` which is the predicted character and `n-1` characters leading up to it.\n",
    "\n",
    "For example if the text is `\"Hello\"`. The input sequence would be `\"Hell\"`, and the target sequence `\"ello\"`.\n",
    "\n",
    "</br>\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary markdown='span'>ü§î Why do we have a target of <strong>ello</strong> if our goal is only to predict <strong>o</strong>? Click here for an explanation.</summary>\n",
    "\n",
    "It is much more stable to train a model this way. If **H** was only updated by the back propagation from the predict of **o** it would be very weakly updated. This problem would be even worse with 100 characters between!\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "‚ùì Write a function `split_input_target` which converts a sequence to a `(input, label)` pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.947952Z",
     "iopub.status.busy": "2022-12-14T13:32:17.947412Z",
     "iopub.status.idle": "2022-12-14T13:32:17.950819Z",
     "shell.execute_reply": "2022-12-14T13:32:17.950262Z"
    },
    "id": "9NGu-FkO_kYU",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def split_input_target(sequence):\n",
    "    input_txt = sequence[:-1]\n",
    "    label_txt = sequence[1:]\n",
    "    \n",
    "    return input_txt, label_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we map the function to our dataset. This applies it to every element in the dataset, this is part of the reason `tensorflow` datasets are so powerful for preprocessing data! üôå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:17.960675Z",
     "iopub.status.busy": "2022-12-14T13:32:17.960150Z",
     "iopub.status.idle": "2022-12-14T13:32:17.998126Z",
     "shell.execute_reply": "2022-12-14T13:32:17.997551Z"
    },
    "id": "B9iKPXkw5xwa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout what our **`dataset`** looks like now üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.001470Z",
     "iopub.status.busy": "2022-12-14T13:32:18.000984Z",
     "iopub.status.idle": "2022-12-14T13:32:18.026512Z",
     "shell.execute_reply": "2022-12-14T13:32:18.025795Z"
    },
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### 2.4) Optimizing the dataset üõ†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tensorflow [datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) we define the batch size before we fit the model. We also:\n",
    "\n",
    "- shuffle the dataset\n",
    "- prefetch (this gets the next N elements ready) - super important when we are loading data from disk to have it ready for the next batch without wasting GPU time! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.029784Z",
     "iopub.status.busy": "2022-12-14T13:32:18.029273Z",
     "iopub.status.idle": "2022-12-14T13:32:18.039620Z",
     "shell.execute_reply": "2022-12-14T13:32:18.039073Z"
    },
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Run the **test** below to check you have a working dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/eren/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/eren/code/erenkiliclar/data-text-generation/tests\n",
      "plugins: anyio-3.6.2, asyncio-0.19.0\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_dataset.py::TestDataset::test_input_shape \u001b[32mPASSED\u001b[0m\u001b[32m                    [ 50%]\u001b[0m\n",
      "test_dataset.py::TestDataset::test_output_shape \u001b[32mPASSED\u001b[0m\u001b[32m                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/dataset.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed dataset step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('dataset', input_shape=tuple(dataset.element_spec[0].shape), output_shape=tuple(dataset.element_spec[1].shape))\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## 3Ô∏è‚É£ Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Define the model üîÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "This section defines the model as a [`keras.Model`](https://keras.io/api/models/model/) subclass which you won't have seen before (For details see [Making new Layers and Models via subclassing](https://www.tensorflow.org/guide/keras/custom_layers_and_models)). \n",
    "\n",
    "This model has three layers:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
    "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
    "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì The **model** is quite different than how we have defined models so far. Take a few minutes to try and understand the code. \n",
    "\n",
    "- The first section is the `__init__` here we define layers using `self.layer_name = layer`\n",
    "- The second section is where we define how to use the layers when we are given an input. You can see we call the layers similarly to the [Keras functional API](https://keras.io/guides/functional_api/) but we the flexibility to include `if` statements and other code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.049811Z",
     "iopub.status.busy": "2022-12-14T13:32:18.049307Z",
     "iopub.status.idle": "2022-12-14T13:32:18.054723Z",
     "shell.execute_reply": "2022-12-14T13:32:18.054087Z"
    },
    "id": "wj8HQ2w8z4iO"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, 256)\n",
    "        self.gru = tf.keras.layers.GRU(1024,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.057787Z",
     "iopub.status.busy": "2022-12-14T13:32:18.057286Z",
     "iopub.status.idle": "2022-12-14T13:32:18.071762Z",
     "shell.execute_reply": "2022-12-14T13:32:18.071185Z"
    },
    "id": "IX58Xj9z47Aw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "<__main__.MyModel object at 0x7f1285ff4d00>\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "print(vocab_size := len(ids_from_chars.get_vocabulary()))\n",
    "\n",
    "print(model := MyModel(vocab_size=vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkA5upJIJ7W7"
   },
   "source": [
    "‚ùóÔ∏è For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "### 3.2) Check the model üî¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets call the untrained model of our first piece of data üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:18.075434Z",
     "iopub.status.busy": "2022-12-14T13:32:18.074933Z",
     "iopub.status.idle": "2022-12-14T13:32:20.720603Z",
     "shell.execute_reply": "2022-12-14T13:32:20.719812Z"
    },
    "id": "C-_70kKAPrPU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "To get actual predictions from the model you need to sample from the output distributions. \n",
    "\n",
    "- This distribution is defined by the logits over the character vocabulary.\n",
    "- ‚ùó It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
    "\n",
    "Try it for the first example in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:20.741781Z",
     "iopub.status.busy": "2022-12-14T13:32:20.741240Z",
     "iopub.status.idle": "2022-12-14T13:32:20.747723Z",
     "shell.execute_reply": "2022-12-14T13:32:20.747167Z"
    },
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[33]\n",
      " [59]\n",
      " [46]\n",
      " [45]\n",
      " [59]\n",
      " [ 3]\n",
      " [35]\n",
      " [61]\n",
      " [52]\n",
      " [18]\n",
      " [32]\n",
      " [52]\n",
      " [23]\n",
      " [35]\n",
      " [ 8]\n",
      " [24]\n",
      " [20]\n",
      " [ 6]\n",
      " [38]\n",
      " [28]\n",
      " [30]\n",
      " [59]\n",
      " [ 8]\n",
      " [26]\n",
      " [10]\n",
      " [42]\n",
      " [32]\n",
      " [20]\n",
      " [ 9]\n",
      " [41]\n",
      " [ 6]\n",
      " [49]\n",
      " [60]\n",
      " [22]\n",
      " [ 6]\n",
      " [12]\n",
      " [ 0]\n",
      " [ 2]\n",
      " [36]\n",
      " [45]\n",
      " [ 0]\n",
      " [22]\n",
      " [46]\n",
      " [26]\n",
      " [12]\n",
      " [ 6]\n",
      " [25]\n",
      " [56]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [15]\n",
      " [ 6]\n",
      " [58]\n",
      " [39]\n",
      " [43]\n",
      " [28]\n",
      " [58]\n",
      " [21]\n",
      " [47]\n",
      " [29]\n",
      " [20]\n",
      " [19]\n",
      " [45]\n",
      " [49]\n",
      " [54]\n",
      " [51]\n",
      " [42]\n",
      " [48]\n",
      " [50]\n",
      " [38]\n",
      " [47]\n",
      " [29]\n",
      " [36]\n",
      " [33]\n",
      " [61]\n",
      " [11]\n",
      " [48]\n",
      " [55]\n",
      " [60]\n",
      " [40]\n",
      " [26]\n",
      " [ 5]\n",
      " [28]\n",
      " [ 9]\n",
      " [41]\n",
      " [ 0]\n",
      " [56]\n",
      " [ 0]\n",
      " [18]\n",
      " [56]\n",
      " [40]\n",
      " [36]\n",
      " [10]\n",
      " [15]\n",
      " [31]\n",
      " [62]\n",
      " [32]\n",
      " [43]\n",
      " [27]\n",
      " [25]], shape=(100, 1), dtype=int64)\n",
      "[33 59 46 45 59  3 35 61 52 18 32 52 23 35  8 24 20  6 38 28 30 59  8 26\n",
      " 10 42 32 20  9 41  6 49 60 22  6 12  0  2 36 45  0 22 46 26 12  6 25 56\n",
      "  4  1 15  6 58 39 43 28 58 21 47 29 20 19 45 49 54 51 42 48 50 38 47 29\n",
      " 36 33 61 11 48 55 60 40 26  5 28  9 41  0 56  0 18 56 40 36 10 15 31 62\n",
      " 32 43 27 25]\n"
     ]
    }
   ],
   "source": [
    "print(sampled_indices := tf.random.categorical(example_batch_predictions[0], num_samples=1))\n",
    "print(sampled_indices := tf.squeeze(sampled_indices, axis=-1).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "This gives us, at each timestep, a prediction of the next character index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:20.751122Z",
     "iopub.status.busy": "2022-12-14T13:32:20.750595Z",
     "iopub.status.idle": "2022-12-14T13:32:20.754941Z",
     "shell.execute_reply": "2022-12-14T13:32:20.754313Z"
    },
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33, 59, 46, 45, 59,  3, 35, 61, 52, 18, 32, 52, 23, 35,  8, 24, 20,\n",
       "        6, 38, 28, 30, 59,  8, 26, 10, 42, 32, 20,  9, 41,  6, 49, 60, 22,\n",
       "        6, 12,  0,  2, 36, 45,  0, 22, 46, 26, 12,  6, 25, 56,  4,  1, 15,\n",
       "        6, 58, 39, 43, 28, 58, 21, 47, 29, 20, 19, 45, 49, 54, 51, 42, 48,\n",
       "       50, 38, 47, 29, 36, 33, 61, 11, 48, 55, 60, 40, 26,  5, 28,  9, 41,\n",
       "        0, 56,  0, 18, 56, 40, 36, 10, 15, 31, 62, 32, 43, 27, 25])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "### 3.3) Train the model üèãÔ∏è‚Äç‚ôÇÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeOXriLcymww"
   },
   "source": [
    "‚ùì Compile the model with the **correct loss** and an optimizer\n",
    "\n",
    "</br>\n",
    "\n",
    "<details>\n",
    "    <summary markdown='span'>üí° Click here for the loss if you're stuck</summary>\n",
    "\n",
    "    You should use the <code>tf.keras.losses.SparseCategoricalCrossentropy</code> loss.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:20.796150Z",
     "iopub.status.busy": "2022-12-14T13:32:20.795668Z",
     "iopub.status.idle": "2022-12-14T13:32:20.913410Z",
     "shell.execute_reply": "2022-12-14T13:32:20.912716Z"
    },
    "id": "DDl1_Een6rL0",
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Run the **test** below to check you have a good model before beginning to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /home/eren/.pyenv/versions/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/eren/code/erenkiliclar/data-text-generation/tests\n",
      "plugins: anyio-3.6.2, asyncio-0.19.0\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_model.py::TestModel::test_loss \u001b[32mPASSED\u001b[0m\u001b[32m                               [ 50%]\u001b[0m\n",
      "test_model.py::TestModel::test_output \u001b[32mPASSED\u001b[0m\u001b[32m                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 2.31s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/model.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed model step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('model', loss=type(model.loss), output_weights=model.trainable_weights[5].shape[0])\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "To keep training within reasonable time, we will use **just 5 epochs** (you can increase this later if you like) to train the model. \n",
    "\n",
    "This will still take about 10 mins so grab a coffee ‚òïÔ∏è while you wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:20.930017Z",
     "iopub.status.busy": "2022-12-14T13:32:20.929797Z",
     "iopub.status.idle": "2022-12-14T13:34:54.171315Z",
     "shell.execute_reply": "2022-12-14T13:34:54.170464Z"
    },
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "172/172 [==============================] - 224s 1s/step - loss: 2.7203\n",
      "Epoch 2/5\n",
      "172/172 [==============================] - 227s 1s/step - loss: 1.9882\n",
      "Epoch 3/5\n",
      "172/172 [==============================] - 261s 2s/step - loss: 1.7067\n",
      "Epoch 4/5\n",
      "172/172 [==============================] - 329s 2s/step - loss: 1.5468\n",
      "Epoch 5/5\n",
      "172/172 [==============================] - 351s 2s/step - loss: 1.4473\n",
      "CPU times: user 2h 5min 4s, sys: 10min 1s, total: 2h 15min 6s\n",
      "Wall time: 23min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## 4Ô∏è‚É£ Generate text üß†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### 4.1) Generation model ü§ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to edit our model for generation, the code below looks excessive so lets break it down:\n",
    "\n",
    "- We will inherit from Keras base model and pass our previously defined model to the `__init__` method\n",
    "- We will also create a mask which add a value of **negative infinity** for the unknown character **`[UNK]`** used to denote characters outside of our vocab as we never want our model to generate this character.\n",
    "- We **sample** and **squeeze** to get the predicted ids.\n",
    "- We pass the state back to allow us to feed it back into the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:34:54.175589Z",
     "iopub.status.busy": "2022-12-14T13:34:54.175313Z",
     "iopub.status.idle": "2022-12-14T13:34:54.183593Z",
     "shell.execute_reply": "2022-12-14T13:34:54.182961Z"
    },
    "id": "iSBU1tHmlUSs"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars):\n",
    "    super().__init__()\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### 4.2) Using the model üìù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9yDoa0G3IgQ"
   },
   "source": [
    "Now we can run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences but pretty impressive for the training time! üôå"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Play around with the input text and the number of predict characters and see what your model creates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T13:34:54.205117Z",
     "iopub.status.busy": "2022-12-14T13:34:54.204676Z",
     "iopub.status.idle": "2022-12-14T13:34:56.855564Z",
     "shell.execute_reply": "2022-12-14T13:34:56.854833Z"
    },
    "id": "ST7PSyk9t1mT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do you cook a roasted chickented\n",
      "is as common me love, and slay died in renatter\n",
      "We'll prunity and so instantion in rear to the\n",
      "bews ow the great are fleshing them:\n",
      "And, if it: and pale I am now;\n",
      "I would then charge meating unstainst me:\n",
      "The dodes your metter land's queen, did beauting thee namus;\n",
      "And surection is, itselves, and death,s\n",
      "Be our meanings, same sire so, yet never frow the exemy couraghment of Henry's name;\n",
      "Who is our lord, bring shall call for the more both.\n",
      "\n",
      "SOMPERNIUS:\n",
      "Hures!\n",
      "\n",
      "CORIOLANUS:\n",
      "What, ''blood!\n",
      "\n",
      "ROMEO:\n",
      "Will, redamed.\n",
      "\n",
      "DUKE OF AUMERLEN:\n",
      "Ances shall go fence; fortunut strangeth,\n",
      "Who has readys lift bain you poft steal'd him dyem\n",
      "Correy lord, but will take or at kind.\n",
      "Wo long the raPs, like a spirity.\n",
      "\n",
      "HASTINGS:\n",
      "I dear he for me.\n",
      "Unon so you shall omenoum.\n",
      "\n",
      "ESBALUS:\n",
      "You are in am;\n",
      "In am I say, may say sedve all haseves are,\n",
      "Me in Godal casing feat:\n",
      "The gosber that nothing me a see, rame angelo,\n",
      "And cautcy to you, if it will not delivers, hatt\n",
      "thou soul to you say\n",
      "Amial, so his Harry, from the \n",
      "\n",
      "________________________________________________________________________________\n",
      "CPU times: user 6.11 s, sys: 1.46 s, total: 7.57 s\n",
      "Wall time: 3.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "states = None\n",
    "next_char = tf.constant(['How do you cook a roasted chicken'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "üèÅ Even though the results could be improved significantly it is quite incredible what the model learnt in **only five** epochs! Next you can up the epochs or try using the model on some text of your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t09eeeR5prIJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2022-12-14T13:32:11.928547Z",
     "iopub.status.busy": "2022-12-14T13:32:11.927961Z",
     "iopub.status.idle": "2022-12-14T13:32:11.931971Z",
     "shell.execute_reply": "2022-12-14T13:32:11.931377Z"
    },
    "id": "GCCk8_dHpuNf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
